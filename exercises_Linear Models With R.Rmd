---
title: "Linear Models with R - Exercises"
author: "Rachel Greenlee"
date: "8/28/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library('faraway')
library('ggplot2')
library('ggExtra')
library('skimr')
library('tidyverse')
```


#Linear Models w/ R Chapter 1


## Exercise 1.1
The dataset teengamb concerns a study of teenage gambling in Britain. Make a
numerical and graphical summary of the data, commenting on any features that
you find interesting. Limit the output you present to a quantity that a busy reader
would find sufficient to get a basic understanding of the data.


```{r}
head(teengamb)
summary(teengamb)
```


```{r}
p1 <- ggplot(teengamb, aes(x = status, y = gamble)) +
  geom_point() +
  labs(title = "Socioeconomic Status by Gambling Expenditures/Year")

ggExtra::ggMarginal(p1, type = "histogram")

```

```{r}
teengamb$sex <- as.factor(teengamb$sex)

ggplot(teengamb, aes(x = sex, y = gamble)) +
  geom_boxplot() +
  labs(title = "Teenager's Sex by Expendirues on Gambling Per Year",
       subtitle = "0 = male, 1 = female")
  
```

```{r}
ggplot(teengamb, aes(x = verbal, y = gamble)) +
  geom_point() +
  labs(title = "Teenager's Verbal Score by Expenditures on Gambling per Year",
       subtitle = "where verbal score = count words out of 12 correctly defined")
```




## Exercise 1.3
The dataset prostate is from a study on 97 men with prostate cancer who were
due to receive a radical prostatectomy. Make a numerical and graphical summary
of the data as in the first question.


```{r}
head(prostate)
summary(prostate)
```





## Exercise 1.4
The dataset sat comes from a study entitled “Getting What You Pay For: The Debate Over Equity in Public School Expenditures.” Make a numerical and graphical
summary of the data as in the first question


```{r}
head(sat)
summary(sat)
```




## Exercise 1.5
The dataset divusa contains data on divorces in the United States from 1920 to
1996. Make a numerical and graphical summary of the data as in the first question.

```{r}
head(divusa)
summary(divusa)
```




#Linear Models w/ R Chapter 2

## Exercise 2.4
The dataset prostate comes from a study on 97 men with prostate cancer who
were due to receive a radical prostatectomy. Fit a model with lpsa as the response
and lcavol as the predictor. Record the residual standard error and the R2. Now
add lweight, svi, lbph, age, lcp, pgg45 and gleason to the model one at a
time. For each model record the residual standard error and the R2. Plot the
trends in these two statistics.

```{r}
# view data summary and pull up help file with variable definitions
summary(prostate)
```


```{r}
# always plot variables before running regression
ggplot(prostate, aes(x = lcavol, y = lpsa)) +
  geom_point()
```


```{r}
# run base regression

lm1 <- lm(lpsa ~ 
            lcavol, data = prostate)
#summary(lm1)

df24 <- data.frame("adjR" = summary(lm1)$adj.r.squared, "residSE" = summary(lm1)$sigma)

# adding one variable at a time & storing adjR and residSE
lm2 <- lm(lpsa ~ 
            lcavol +
            lweight, data = prostate)
df24 <- rbind(df24, list(summary(lm2)$adj.r.squared, summary(lm2)$sigma))


lm3 <- lm(lpsa ~ 
            lcavol +
            lweight +
            svi, data = prostate)
df24 <- rbind(df24, list(summary(lm3)$adj.r.squared, summary(lm3)$sigma))


lm4 <- lm(lpsa ~ 
            lcavol +
            lweight +
            svi +
            lbph, data = prostate)
df24 <- rbind(df24, list(summary(lm4)$adj.r.squared, summary(lm4)$sigma))


lm5 <- lm(lpsa ~ 
            lcavol +
            lweight +
            svi +
            lbph +
            age, data = prostate)
df24 <- rbind(df24, list(summary(lm5)$adj.r.squared, summary(lm5)$sigma))


lm6 <- lm(lpsa ~ 
            lcavol +
            lweight +
            svi +
            lbph +
            age +
            lcp, data = prostate)
df24 <- rbind(df24, list(summary(lm6)$adj.r.squared, summary(lm6)$sigma))


lm7 <- lm(lpsa ~ 
            lcavol +
            lweight +
            svi +
            lbph +
            age +
            lcp +
            pgg45, data = prostate)
df24 <- rbind(df24, list(summary(lm7)$adj.r.squared, summary(lm7)$sigma))

lm8 <- lm(lpsa ~ 
            lcavol +
            lweight +
            svi +
            lbph +
            age +
            lcp +
            pgg45 +
            gleason, data = prostate)
df24 <- rbind(df24, list(summary(lm8)$adj.r.squared, summary(lm8)$sigma))

```


### It appears the adjusted R-squared and the residual standard error have a perfect linear relationship.

```{r}
# plotting the stored adjusted R-squared and residual standard error from each
# model

df24$vars <- 1:nrow(df24)

ggplot(df24, aes(x = vars, y = residSE)) +
  geom_point()

ggplot(df24, aes(x = vars, y = adjR)) +
  geom_point()

ggplot(df24, aes(x = adjR, y = residSE)) +
  geom_point()
```


## Exercise 2.5
Using the prostate data, plot lpsa against lcavol. Fit the regressions of lpsa
on lcavol and lcavol on lpsa. Display both regression lines on the plot. At
what point do the two lines intersect?


```{r}
lmA <- lm(lcavol ~ lpsa, data = prostate)
lmB <- lm(lpsa ~ lcavol, data = prostate)

ggplot(prostate, aes(lcavol, lpsa)) +
  geom_point() +
  geom_line(aes(x = predict(lmA), color = "lcavol ~ lpsa")) +
  geom_line(aes(y = predict(lmB), color = "lpsa ~ lcavol"))


```



#Linear Models w/ R Chapter 3
## Exercise 3.4

4. Using the sat data:
(a) Fit a model with total sat score as the response and expend, ratio and
salary as predictors. Test the hypothesis that βsalary = 0. Test the hypothesis
that βsalary = βratio = βexpend = 0. Do any of these predictors have an
effect on the response?


```{r}
head(sat)
```


```{r}
# model with 3 predictors
sat_lm1 <- lm(total ~ expend + ratio + salary, data = sat)
summary(sat_lm1)
```

The hypothesis that βsalary = 0 is rejected due to a low p-value of 0.002

```{r}
# test null hypothesis that Beta_salary = 0
# meaning that the salary alone doesn't have any slope/affect on total avg score
sat_lm2 <- lm(total ~ salary, data = sat)
summary(sat_lm2)
```


The null hypothesis that βsalary = βratio = βexpend = 0 is rejected due to the small p-value of 0.0121.

```{r}
# create model where slope = 1 (aka no variables predicting)
sat_nullmod <- lm(total ~ 1, sat)

# compare null to our model created above
anova(sat_nullmod, sat_lm1)
```



(b) Now add takers to the model. Test the hypothesis that βtakers = 0. Compare
this model to the previous one using an F-test. Demonstrate that the F-test and
t-test here are equivalent.

```{r}
# model with 4 predictors
sat_lm3 <- lm(total ~ expend + ratio + salary + takers, data = sat)
summary(sat_lm3)
```


The hypothesis that βtakers = 0 is rejected due to a low p-value of nearly zero.


```{r}
# test null hypothesis that Beta_takers = 0
# meaning that the takers alone doesn't have any slope/affect on total avg score
sat_lm4 <- lm(total ~ takers, data = sat)
summary(sat_lm4)
```

The test shows that the model with the addition of 'takers' is justifiable due to the small p-value.


```{r}
anova(sat_lm1, sat_lm3)
```



#Linear Models w/ R Chapter 4
## Exercise 4.5

5. For the fat data used in this chapter, a smaller model using only age, weight,
height and abdom was proposed on the grounds that these predictors are either
known by the individual or easily measured.

```{r}
fat_lm0 <- lm(brozek ~ age + weight + height + neck + chest + abdom + hip + 
                thigh + knee + ankle + biceps + forearm + wrist, data=fat)
fat_lm1 <- lm(brozek ~ age + weight + height + abdom, data = fat)
```



(a) Compare this model to the full thirteen-predictor model used earlier in the
chapter. Is it justifiable to use the smaller model?  


Since the p-value is small the null hypothesis is rejected. Removing all of the predictors is not justifiable.

We were essentially testing a H0: β's of the removed predictors = 0. They do not equal zero, so we can't remove them (may be better to test one at a time).

```{r}
anova(fat_lm1, fat_lm0)
```



(b) Compute a 95% prediction interval for median predictor values and compare
to the results to the interval for the full model. Do the intervals differ by a
practically important amount?  


Full model is body fat between 9.62% and 25.37%.  

Smaller model is body fat between 9.7% and 25.98%.  
I would argue they do not differ by an practically important amount a user would care about.

```{r}

# grab model and get median coefficients in format the predict func will like
fat_lm0_matrix <- model.matrix(fat_lm0)
(fat_lm0_medians <- apply(fat_lm0_matrix, 2, median))
predict(fat_lm0, new = data.frame(t(fat_lm0_medians)), interval = "prediction")

# and again for the smaller model
fat_lm1_matrix <- model.matrix(fat_lm1)
(fat_lm1_medians <- apply(fat_lm1_matrix, 2, median))
predict(fat_lm1, new = data.frame(t(fat_lm1_medians)), interval = "prediction")

```



(c) For the smaller model, examine all the observations from case numbers 25 to
50. Which two observations seem particularly anomalous?  

Case numbers 47 & 50 have negative body fat %, which isn't possible.

```{r}
new_df_25_50 <- fat[25:50,c(4,5,6,11)]
predict(fat_lm1, new=data.frame(new_df_25_50), interval="prediction")
```




(d) Recompute the 95% prediction interval for median predictor values after these
two anomalous cases have been excluded from the data. Did this make much
difference to the outcome?

The prediction interval is 9.71% to 26.04%. This is a bit smaller than before we removed those 2 troublesome cases.

```{r}
fat_cleaned <- fat[-c(47, 50),]

fat_lm1_cleaned <- lm(brozek ~ age + weight + height + abdom, data = fat_cleaned)

fat_lm1_cleaned_matrix <- model.matrix(fat_lm1_cleaned)
(fat_lm1_cleaned_medians <- apply(fat_lm1_cleaned_matrix, 2, median))
predict(fat_lm1_cleaned, new = data.frame(t(fat_lm1_cleaned_medians)), interval = "prediction")
```



#Linear Models w/ R Chapter 5
## Exercise 5.2  

Use the odor dataset with odor as the response and temp as a predictor. Consider
all possible models that also include all, some or none of the other two predictors.
Report the coefficient for temperature, its standard error, t-statistic and p-value in
each case. Discuss what stays the same, what changes and why. Which model is
best?

```{r}
head(odor)
```


None of the Adjusted R-Squared in the 4 models are all that good, some are even negative. No significant p-values. 

```{r}
odor_lm0 <- lm(odor ~ temp, data = odor)
odor_lm_some <- lm(odor ~ temp + gas, data = odor)
odor_lm_some2 <- lm(odor ~ pack + gas, data = odor)
odor_lm_all <- lm(odor ~ temp + gas + pack, data = odor)

summary(odor_lm0)
summary(odor_lm_some)
summary(odor_lm_some2)
summary(odor_lm_all)

```





#Linear Models w/ R Chapter 14
## Exercise 14.2  

Using the infmort data, find a simple model for the infant mortality in terms of
the other variables. Be alert for transformations and unusual points. Interpret your
model by explaining what the regression parameter estimates mean.

*Note we haven't read the chapter on transformations yet so I may be missing something here*

```{r}
head(infmort)
skim(infmort)

ggplot(infmort, aes(x = income)) +
  geom_histogram()

ggplot(infmort, aes(x = mortality)) +
  geom_histogram()

ggplot(infmort, aes(x = region)) +
  geom_histogram(stat = "count")

ggplot(infmort, aes(x = oil)) +
  geom_histogram(stat = "count")

```
The full model shows income is not significant, but that oil and region might be. 

```{r}
inf_fullmod <- lm(mortality ~ income + oil + region, data = infmort)
summary(inf_fullmod)
```


Checking to see what each one does on it's own, income is once again not significant.

```{r}
drop1(inf_fullmod, test = "F")
```

```{r}
inf_partialmod <- lm(mortality ~ region + oil, data = infmort)
summary(inf_partialmod)
```
Not a ton of improvement. Lets try region and oil as an interaction term.

```{r}
inf_partialmod_int <- lm(mortality ~ region:oil, data = infmort)
summary(inf_partialmod_int)
```

This shows a slightly lower residual standard error, meaning our predictions are better, and the adjusted r-squared increased. 



#Linear Model w/ R Chapter 6
## Exercise 6.1

1. Using the sat dataset, fit a model with the total SAT score as the response and
expend, salary, ratio and takers as predictors. Perform regression diagnostics on
this model to answer the following questions. Display any plots that are relevant.
Do not provide any plots about which you have nothing to say. Suggest possible
improvements or corrections to the model where appropriate.


```{r}
?sat
```

```{r}
sat_model <- lm(total ~ expend + salary + ratio + takers, data = sat)
summary(sat_model)
```



(a) Check the constant variance assumption for the errors.



On the residuals vs fitted plot does appear a bit curved, such that the residuals are more positive on the lower and higher extremes of the x-values. This is slightly concerning so we will look at the square root of the absolute value of the errors, as this effectively doubles the resolution of the residuals to see more clearly.


```{r}
plot(fitted(sat_model), residuals(sat_model), xlab = "Fitted", ylab = "Residuals")
abline(h=0)

```

This appears to have more constant variance and I don't believe there is any reason to be concerned. 

```{r}
plot(fitted(sat_model), sqrt(abs(residuals(sat_model))), xlab = "Fitted", ylab = expression(sqrt(hat(epsilon))))

```



(b) Check the normality assumption.  

There is some flaring toward the highest end of x-values, but otherwise the normality assumption is well-met. 


```{r}
qqnorm(residuals(sat_model), ylab = "Residuals", main = "")
qqline(residuals(sat_model))
```



(c) Check for large leverage points.

A rough rule is that leverages of more than 2p/n should be looked at more closely. In this case that value would be 2 * 4 (predictors) / 50 (n) = 0.16. Looking at the top 10 highest leverage values, the top 7 are larger than this value so we will take a closer look with a half-normal plot (best for positive values).



```{r}
hatv <- hatvalues(sat_model)

head(sort(hatv, decreasing = TRUE), 10)
sum(hatv)
```



We see Utah and California listed as being outliers, but do seem to fall in the trend of the rest of the data despite being very extreme x-values. 


```{r}
states <- row.names(sat)
halfnorm(hatv, labs = states, ylab = "Leverages")
```

Looking at this Q-Q plot of the standardized residuals, things are less concerning. Since the residuals have now been standardized, we expect the points to roughly follow this x = y line, which it does, meaning normality is holding here. Further, we don't have any absolute values beyond 2, which is to be expected under the standard normal.  

**Conclude that there are no leverage points affecting the model's slope.**

```{r}
qqnorm(rstandard(sat_model))
abline(0,1)
```



(d) Check for outliers.  

Using the Bonferroni correction we can compute if a value is a outlier. The general formula is qt(1-alpha/2, n-p).



```{r}
stud <- rstudent(sat_model)
stud[which.max(abs(stud))]

qt(p = 0.05/(50*2), df = 46)

```



(e) Check for influential points.  

An influential point is one whose removal from the dataset would cause a large change in in the fit. The Cook statistics are popular influence diagnostics because they reduce the information to a single value for each case. We see that Utah, West Virginia, and New Hampshire are the top 3 influential observations. 

```{r}
cook <- cooks.distance(sat_model)
halfnorm(cook, 3, labs = states, y.lab = "Cook's distances")
```

To see the model with Utah removed. Compared to our original model, we see the coefficients for expend, salary, and ratio all changed by large magnitudes which is disconcerting that the estimates are so sensitive to just one state. 

```{r}
sat_model_no_utah <- lm(total ~ expend + salary + ratio + takers, data = sat, subset = (cook < max(cook)))
summary(sat_model_no_utah)
summary(sat_model)

```


```{r}
plot(dfbeta(sat_model)[,4], ylab = "Change in ratio coef")
abline(h=0)
### I can't get the identify() function to work to see outliers here
```






















#Linear Models w/ R Chapter 9
## Exercise 9.1
The aatemp data come from the U.S. Historical Climatology Network. They are
the annual mean temperatures (in degrees F) in Ann Arbor, Michigan going back
about 150 years.  

```{r}
?aatemp
```


(a) Is there a linear trend?   

From this basic scatterplot it does appear there could be a positive linear trend. When adding the geom_smooth's attempt at a linear model line there is a slight positive linear trend.

```{r}
ggplot(aatemp, aes(x = year, y = temp)) +
  geom_point() +
  geom_smooth(method = "lm")
```


(b) Observations in successive years may be correlated. Fit a model that estimates
this correlation. Does this change your opinion about the trend?  
**Haven't covered this concept in book yet**

(c) Fit a polynomial model with degree 10 and use backward elimination to reduce
the degree of the model. Plot your fitted model on top of the data. Use this
model to predict the temperature in 2020.  
**Haven't covered this concept in book yet**

(d) Suppose someone claims that the temperature was constant until 1930 and then
began a linear trend. Fit a model corresponding to this claim. What does the
fitted model say about this claim?  
**Haven't covered this concept in book yet**

(e) Make a cubic spline fit with six basis functions evenly spaced on the range.
Plot the fit in comparison to the previous fits. Does this model fit better than
the straight-line model?  
**Haven't covered this concept in book yet**

## Exercise 9.3
Using the ozone data, fit a model with O3 as the response and temp, humidity and
ibh as predictors. Use the Box–Cox method to determine the best transformation
on the response.  

```{r}

ozone_lm <- lm(O3 ~ temp + humidity + ibh, data = ozone)
```

```{r}
require(MASS)
boxcox(ozone_lm, plotit=T)

# zoom in on the confidence interval
boxcox(ozone_lm, plotit=T,
       lambda = seq(0, 0.5, by = 0.1))

```


#Linear Models w/ R Chapter 13
## Exercise 13.1
The dataset kanga contains data on the skulls of historical kangaroo specimens.
Ignore the sex and species variables for the purposes of this question.  

(a) Report on the distribution of missing values within the data according to case
and variable.  
From the table below we can see that 101 of the cases have no  missing values, 17 are missing only palate.width, 10 are missing mandible.length, and so on. Looking on the far right column for the totals, there are no cases missing more than 3 values. This is useful for seeing combinations/patterns of missing values.  

If we sum the left column, except for the 101 complete cases, we can see that if we were to delete any incomplete cases we would lose 47 of 148 cases, 32% of the original data.

```{r}

library(mice)
library(kableExtra)
kable((md.pattern(kanga, plot = FALSE)), format = "html")
```

I also like using the skim function. The complete rate only shows palate.width with a missing rate below 90% (at 84%) and 24 missing cases. The rest are not too concerning.

```{r}
library(skimr)
fix_windows_histograms() #otherwise histograms display as unicode
skim(kanga)
```


(b) Determine a combination of variable and case deletions that retains the most
information.  

First we'll delete any case with a missing value and see the correlation plot for any collinearity in variables.  

For palate.width, for which 24 cases have an NA, the correlation plot shows very strong correlations (0.8) with zygomatic.width, occipital.depth, and mandible.length. This is high enough to suggest collinearity and since these other variables will likely carry any prediction load, I choose to simply remove the palate.width variable from the original kanga dataset.

```{r, fig.height=10, fig.width = 8}
# remove any row with an NA in any column
kanga_complete <- na.omit(kanga)

# remove two categorical variables sex and species
kanga_complete_corr <- subset(kanga_complete, select = -c(species, sex))

# correlation plot of complete cases from kanga
library(ggcorrplot)
corr <- round(cor(kanga_complete_corr), 1)
ggcorrplot(corr, method = "circle", type = "upper", lab = TRUE, insig = "blank")

# remove palate.width from original kanga set
kanga_slim <- subset(kanga, select = -c(palate.width))


```

Since the rest of the missing data appears to be random and never accounts for more than 10% of cases, I will impute the remaining values using a regression fill-in method. The textbook notes (page 202) that this method works best when the predictors have more colinearity, which is certainly the case in this dataset as seen in the correlation plot above.

```{r}
# impute using linear regression
kanga_imputed <- complete(mice(data = kanga_slim, m = 1,
                               method = "norm.predict"))

# check to see now there are no NAs
skim(kanga_imputed)
```



c) Compute the PCA, omitting all missing cases, where all the variables have
been scaled to have SD 1. Report on the standard deviation of the principal
components.  
**Not covered in this class yet.**  

(d) Perform multiple imputation 25 times, applying PCA in the same manner to
each of the imputed datasets. Combine the standard deviations of the principal
components in the imputed datasets.  
**Not covered in this class yet.**

## Exercise 13.2
2. The dataset galamiss contains the Galapagos data frequently featured as an example
in this text as gala but with the original missing values left in.  
**Unfortunately dataset doesn't exist.**


(a) Fit a linear model using gala with the number of species as the response and
the five geographic predictors as in earlier examples.  


(b) Fit the same model to galamiss using the deletion strategy for missing values.
Compare the fit to that in (a).  

(c) Use mean value single imputation on galamiss and again fit the model. Compare
to previous fits.  

(d) Use a regression-based imputation using the other four geographic predictors
to fill in the missing values in galamiss. Fit the same model and compare to
previous fits.  

(e) Use multiple imputation to handle missing values and fit the same model again.
Compare to previous fits.  

#Linear Models w/ R Chapter 7
## Exercise 7.3
3. Using the divusa data:
(a) Fit a regression model with divorce as the response and unemployed, femlab,
marriage, birth and military as predictors. Compute the condition
numbers and interpret their meanings.  


```{r}
#?divusa

div_mod <- lm(divorce ~ unemployed + femlab + marriage + birth + military, data = divusa)
summary(div_mod)

```

Looking at the pairwise correlations below none of the variables in the model seem too correlated.

```{r}
round(cor(divusa[,-9]), 2)
```

The eigenvalues have some range, but the condition numbers are not too large.

```{r}
# taken from book page 108
x <- model.matrix(div_mod)[,-1]
e <- eigen(t(x) %*% x)
e$val

sqrt(e$val[1]/e$val)
```


(b) For the same model, compute the VIFs. Is there evidence that collinearity
causes some predictors not to be significant? Explain.  

VIF, or variance inflation factor, is a simple measure of the harm produced by collinearity. The square root of the VIF indicates how much the confidence interval for Beta-j is expanded relative to similar uncorrelated data, were it possible for such data to exist.


Compared to the example in the book with some values in the hundred, these numbers are low and not concerning. If you were to take the square root of one of the below values, we would say, "the standard error for unemployed is x times larger than it would have been without collinearity". 

"Variance inflation factors exceeding 4 indicate that confidence intervals for these coefficients are more than twice as wide as they would be for uncorrelated predictors" - p433 An R Companion to Applied Regression 3rd Edition

None of the values below are larger than 4. 

```{r}
# in faraway package
vif(x)
```


(c) Does the removal of insignificant predictors from the model reduce the collinearity?
Investigate.

**I don't believe there is any to start with.**
