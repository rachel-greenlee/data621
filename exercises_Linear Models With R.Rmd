---
title: "Linear Models with R - Exercises"
author: "Rachel Greenlee"
date: "8/28/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library('faraway')
library('ggplot2')
library('ggExtra')
```


#Linear Models w/ R Chapter 1


## Exercise 1.1
The dataset teengamb concerns a study of teenage gambling in Britain. Make a
numerical and graphical summary of the data, commenting on any features that
you find interesting. Limit the output you present to a quantity that a busy reader
would find sufficient to get a basic understanding of the data.


```{r}
head(teengamb)
summary(teengamb)
```


```{r}
p1 <- ggplot(teengamb, aes(x = status, y = gamble)) +
  geom_point() +
  labs(title = "Socioeconomic Status by Gambling Expenditures/Year")

ggExtra::ggMarginal(p1, type = "histogram")

```

```{r}
teengamb$sex <- as.factor(teengamb$sex)

ggplot(teengamb, aes(x = sex, y = gamble)) +
  geom_boxplot() +
  labs(title = "Teenager's Sex by Expendirues on Gambling Per Year",
       subtitle = "0 = male, 1 = female")
  
```

```{r}
ggplot(teengamb, aes(x = verbal, y = gamble)) +
  geom_point() +
  labs(title = "Teenager's Verbal Score by Expenditures on Gambling per Year",
       subtitle = "where verbal score = count words out of 12 correctly defined")
```




## Exercise 1.3
The dataset prostate is from a study on 97 men with prostate cancer who were
due to receive a radical prostatectomy. Make a numerical and graphical summary
of the data as in the first question.


```{r}
head(prostate)
summary(prostate)
```





## Exercise 1.4
The dataset sat comes from a study entitled “Getting What You Pay For: The Debate Over Equity in Public School Expenditures.” Make a numerical and graphical
summary of the data as in the first question


```{r}
head(sat)
summary(sat)
```




## Exercise 1.5
The dataset divusa contains data on divorces in the United States from 1920 to
1996. Make a numerical and graphical summary of the data as in the first question.

```{r}
head(divusa)
summary(divusa)
```




#Linear Models w/ R Chapter 2

## Exercise 2.4
The dataset prostate comes from a study on 97 men with prostate cancer who
were due to receive a radical prostatectomy. Fit a model with lpsa as the response
and lcavol as the predictor. Record the residual standard error and the R2. Now
add lweight, svi, lbph, age, lcp, pgg45 and gleason to the model one at a
time. For each model record the residual standard error and the R2. Plot the
trends in these two statistics.

```{r}
# view data summary and pull up help file with variable definitions
summary(prostate)
```


```{r}
# always plot variables before running regression
ggplot(prostate, aes(x = lcavol, y = lpsa)) +
  geom_point()
```


```{r}
# run base regression

lm1 <- lm(lpsa ~ 
            lcavol, data = prostate)
#summary(lm1)

df24 <- data.frame("adjR" = summary(lm1)$adj.r.squared, "residSE" = summary(lm1)$sigma)

# adding one variable at a time & storing adjR and residSE
lm2 <- lm(lpsa ~ 
            lcavol +
            lweight, data = prostate)
df24 <- rbind(df24, list(summary(lm2)$adj.r.squared, summary(lm2)$sigma))


lm3 <- lm(lpsa ~ 
            lcavol +
            lweight +
            svi, data = prostate)
df24 <- rbind(df24, list(summary(lm3)$adj.r.squared, summary(lm3)$sigma))


lm4 <- lm(lpsa ~ 
            lcavol +
            lweight +
            svi +
            lbph, data = prostate)
df24 <- rbind(df24, list(summary(lm4)$adj.r.squared, summary(lm4)$sigma))


lm5 <- lm(lpsa ~ 
            lcavol +
            lweight +
            svi +
            lbph +
            age, data = prostate)
df24 <- rbind(df24, list(summary(lm5)$adj.r.squared, summary(lm5)$sigma))


lm6 <- lm(lpsa ~ 
            lcavol +
            lweight +
            svi +
            lbph +
            age +
            lcp, data = prostate)
df24 <- rbind(df24, list(summary(lm6)$adj.r.squared, summary(lm6)$sigma))


lm7 <- lm(lpsa ~ 
            lcavol +
            lweight +
            svi +
            lbph +
            age +
            lcp +
            pgg45, data = prostate)
df24 <- rbind(df24, list(summary(lm7)$adj.r.squared, summary(lm7)$sigma))

lm8 <- lm(lpsa ~ 
            lcavol +
            lweight +
            svi +
            lbph +
            age +
            lcp +
            pgg45 +
            gleason, data = prostate)
df24 <- rbind(df24, list(summary(lm8)$adj.r.squared, summary(lm8)$sigma))

```


### It appears the adjusted R-squared and the residual standard error have a perfect linear relationship.

```{r}
# plotting the stored adjusted R-squared and residual standard error from each
# model

df24$vars <- 1:nrow(df24)

ggplot(df24, aes(x = vars, y = residSE)) +
  geom_point()

ggplot(df24, aes(x = vars, y = adjR)) +
  geom_point()

ggplot(df24, aes(x = adjR, y = residSE)) +
  geom_point()
```


## Exercise 2.5
Using the prostate data, plot lpsa against lcavol. Fit the regressions of lpsa
on lcavol and lcavol on lpsa. Display both regression lines on the plot. At
what point do the two lines intersect?


```{r}
lmA <- lm(lcavol ~ lpsa, data = prostate)
lmB <- lm(lpsa ~ lcavol, data = prostate)

ggplot(prostate, aes(lcavol, lpsa)) +
  geom_point() +
  geom_line(aes(x = predict(m), color = "lcavol ~ lpsa")) +
  geom_line(aes(y = predict(m2), color = "lpsa ~ lcavol"))


```



#Linear Models w/ R Chapter 3
## Exercise 3.4

4. Using the sat data:
(a) Fit a model with total sat score as the response and expend, ratio and
salary as predictors. Test the hypothesis that βsalary = 0. Test the hypothesis
that βsalary = βratio = βexpend = 0. Do any of these predictors have an
effect on the response?


```{r}
head(sat)
```


```{r}
# model with 3 predictors
sat_lm1 <- lm(total ~ expend + ratio + salary, data = sat)
summary(sat_lm1)
```

The hypothesis that βsalary = 0 is rejected due to a low p-value of 0.002

```{r}
# test null hypothesis that Beta_salary = 0
# meaning that the salary alone doesn't have any slope/affect on total avg score
sat_lm2 <- lm(total ~ salary, data = sat)
summary(sat_lm2)
```


The null hypothesis that βsalary = βratio = βexpend = 0 is rejected due to the small p-value of 0.0121.

```{r}
# create model where slope = 1 (aka no variables predicting)
sat_nullmod <- lm(total ~ 1, sat)

# compare null to our model created above
anova(sat_nullmod, sat_lm1)
```



(b) Now add takers to the model. Test the hypothesis that βtakers = 0. Compare
this model to the previous one using an F-test. Demonstrate that the F-test and
t-test here are equivalent.

```{r}
# model with 4 predictors
sat_lm3 <- lm(total ~ expend + ratio + salary + takers, data = sat)
summary(sat_lm3)
```


The hypothesis that βtakers = 0 is rejected due to a low p-value of nearly zero.


```{r}
# test null hypothesis that Beta_takers = 0
# meaning that the takers alone doesn't have any slope/affect on total avg score
sat_lm4 <- lm(total ~ takers, data = sat)
summary(sat_lm4)
```

The test shows that the model with the addition of 'takers' is justifiable due to the small p-value.


```{r}
anova(sat_lm1, sat_lm3)
```



#Linear Models w/ R Chapter 4
## Exercise 4.5

5. For the fat data used in this chapter, a smaller model using only age, weight,
height and abdom was proposed on the grounds that these predictors are either
known by the individual or easily measured.

```{r}
fat_lm0 <- lm(brozek ~ age + weight + height + neck + chest + abdom + hip + 
                thigh + knee + ankle + biceps + forearm + wrist, data=fat)
fat_lm1 <- lm(brozek ~ age + weight + height + abdom, data = fat)
```



(a) Compare this model to the full thirteen-predictor model used earlier in the
chapter. Is it justifiable to use the smaller model?  


Since the p-value is small the null hypothesis is rejected. Removing all of the predictors is not justifiable.

We were essentially testing a H0: β's of the removed predictors = 0. They do not equal zero, so we can't remove them (may be better to test one at a time).

```{r}
anova(fat_lm1, fat_lm0)
```



(b) Compute a 95% prediction interval for median predictor values and compare
to the results to the interval for the full model. Do the intervals differ by a
practically important amount?  


Full model is body fat between 9.62% and 25.37%.  

Smaller model is body fat between 9.7% and 25.98%.  
I would argue they do not differ by an practically important amount a user would care about.

```{r}

# grab model and get median coefficients in format the predict func will like
fat_lm0_matrix <- model.matrix(fat_lm0)
(fat_lm0_medians <- apply(fat_lm0_matrix, 2, median))
predict(fat_lm0, new = data.frame(t(fat_lm0_medians)), interval = "prediction")

# and again for the smaller model
fat_lm1_matrix <- model.matrix(fat_lm1)
(fat_lm1_medians <- apply(fat_lm1_matrix, 2, median))
predict(fat_lm1, new = data.frame(t(fat_lm1_medians)), interval = "prediction")

```



(c) For the smaller model, examine all the observations from case numbers 25 to
50. Which two observations seem particularly anomalous?  

Case numbers 47 & 50 have negative body fat %, which isn't possible.

```{r}
new_df_25_50 <- fat[25:50,c(4,5,6,11)]
predict(fat_lm1, new=data.frame(new_df_25_50), interval="prediction")
```




(d) Recompute the 95% prediction interval for median predictor values after these
two anomalous cases have been excluded from the data. Did this make much
difference to the outcome?

The prediction interval is 9.71% to 26.04%. This is a bit smaller than before we removed those 2 troublesome cases.

```{r}
fat_cleaned <- fat[-c(47, 50),]

fat_lm1_cleaned <- lm(brozek ~ age + weight + height + abdom, data = fat_cleaned)

fat_lm1_cleaned_matrix <- model.matrix(fat_lm1_cleaned)
(fat_lm1_cleaned_medians <- apply(fat_lm1_cleaned_matrix, 2, median))
predict(fat_lm1_cleaned, new = data.frame(t(fat_lm1_cleaned_medians)), interval = "prediction")
```



#Linear Models w/ R Chapter 5
## Exercise 5.2


#Linear Models w/ R Chapter 14
## Exercise 14.2