---
title: "Wine Sales"
author: "Biguzzi, Connin, Greenlee, Moscoe, Sooklall, Telab, and Wright"
date: "11/05/2021"
output:
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, fig.width = 8, out.width = "100%")
```

```{r, include = FALSE}
library(dplyr)
library(flextable)
library(dlookr) # for data exploration & imputation
library(corrplot) # for visualization variable correlations
library(ggplot2)
library(tidyr) # for gather function
library(MASS) # for glm.nb function at negative binomial

raw <- read.csv("https://raw.githubusercontent.com/rachel-greenlee/data621/main/HW5_count/wine-training-data.csv")
eval <- read.csv("https://raw.githubusercontent.com/rachel-greenlee/data621/main/HW5_count/wine-evaluation-data.csv")


```


## Introduction

Using the information about sample wine orders by restaurants and wine stores after a tasting, how can we predict wine sales by various wine characteristics? Using the `wine` dataset with 12,000 entries and variables mostly related to the chemical properties of each wine, we will build a count regression model to predict the number of cases of wine that will be sold. In practice, if a wine manufacturer can predict which wines will lead to greater sales, they can choose to offer those at more tastings in restaurants and wine stores.  

Using the training data set we will build:  

* two different poisson regression models
* two different negative binomial regression models
* two different multiple linear regression models

In this report we will:

* explore the data
* transform the data to meet conditions of count modeling
* compare models
* select an optimal model
* generate predictions for the evaluation data set


## Data Exploration

As part of our initial data exploration below, we find the following issues that will be handled in the Data Preparation section: 

* large amounts of negative values for 8 of the chemical measures (which must be in error)
* 26.25% of cases have a missing `STARS` value
* `Label Appeal` and `STARS` have imported as integers, but as could be interpreted as categorical variables due to lack of continuity
* many of the chemical variables could benefit from log transformations, after seeing the normality plots
* very few variables are correlated with the `TARGET` beyond `STARS` and `LabelAppeal`, consider dropping some


### First Look

Taking a look at the structure of the dataset we have 12,795 cases and 14 potential predictor variables. All variables are numeric. We'll remove the `INDEX` variable as it isn't needed in model building.

```{r}
str(raw)

# remove first column, INDEX
raw <- raw[-1]
```


The table below shows the range of the TARGET (# cases purchased) ranges from 0 - 8. We also see a large amount of negative values across some of the chemical variables. We will need to deal with these values and/or cases later as it isn't possible for wine to have negative values in any of these chemical measures.

```{r}
raw %>%
    diagnose_numeric() %>%
    dplyr::select(variables, min, mean, median, max, zero, minus) %>%
    flextable(theme_fun = theme_booktabs()) %>%
    colformat_double(digits = 2)
```


### Checking for Normality


Look at the distribution of all of the predictor variables with boxplots.
```{r}
raw %>%
  gather() %>%
  ggplot(aes(value)) +
  facet_wrap(~ key, scales = "free", ncol = 3) +
  geom_boxplot() +
  ggthemes::theme_fivethirtyeight()

```




Using the Shapiro-Wilk normality test on each of the variables, we see all of the p-values are less than 0.05 which means none of our variables are normally distributed in their raw form.

```{r}
raw %>% 
  normality() %>%
  arrange(desc(p_value)) %>%
    flextable(theme_fun = theme_booktabs()) %>%
    colformat_double(digits = 3)

```


Below we visualize with a histogram and Q-Q plot for each variable for which we want to check for normality, excluding the `TARGET` variable and variables we will conver to factors later. It appears that all 11 of the variables would benefit from a log transformation, which will likely become even more necessary once we correct for the un-interpretable negative values many of these variables have in the original dataset.

```{r}
# normality plots for all numeric dependent variables
raw %>%
  dplyr::select(-TARGET, -LabelAppeal, -STARS, -AcidIndex) %>%
  plot_normality()
```



### Missingness

With regards to missingness, we have 6 variables with data for every case. This leaves 8 variables that have some degree of missing data, with the worst being the `STARS` variable with 26.25% of cases missing a value for `STARS` (the wine rating by experts). None of the remaining 7 variables have more than 10% missing, so we will not consider dropping or imputing missing values for them.

```{r}
# check for missing values
raw %>%
    diagnose() %>%
    dplyr::select(-unique_count, -unique_rate) %>%
    filter(missing_count > 0) %>%
    arrange(desc(missing_count)) %>%
    flextable(cwidth = 1.2) %>%
    colformat_double(digits = 2)
```



Looking for patterns in missingness can be telling. Below the 8 variables with at least some missingness are plotted for the top 30 most frequent combinations of missing values.  

We don't see any major relationships of missingness that affect a large number of cases in the dataset.
```{r, fig.height=6}

# plotting patterns in missingness of top 6 most-missing variables
raw %>% 
  plot_na_intersect(only_na = TRUE, typographic = FALSE, n_intersacts = 30)

```

### Correlation

We see a very sparase correlation plot below. `LabelAppeal` and `STARS` jump out as having the largest positive correlation with `TARGET`, with a faint negative correlation between `AcidIndex` and the `TARGET`. We also see a correlation between `LabelAppeal` and `STARS`.

```{r}
# create corr values
correlation <- cor(raw, use = "complete.obs") 

# plot correlations
corrplot.mixed(correlation, tl.col = 'black', tl.pos = 'lt', number.cex= 11/ncol(raw))
```



### Check for Outliers

The table below shows the count and perfect of outliers within each variable arranged with the highest outlier count starting at the top. The outliers_ratio shows the percent of outliers identified withing all cases, which is greater than 5% for the first 12 variables. We will recheck that table after transformations in the Data Preparation section.

```{r}
diagnose_outlier(raw) %>%
  arrange(desc(outliers_cnt)) %>% 
  mutate_if(is.numeric, round , digits=3) %>% 
  flextable() 
```



## Data Preparation

To address the issues in the data we discovered in the data exploration section, we will:

* **CHANGE DATA TYPE** to factors for `STARS`, interpreting as categorical variables due to lack of continuity
* **Bucket `LabelAppeal` Ratings** into Negative, Neutral, and Positive scores
* **TAKE ABSOLUTE VALUE of all negative values** for 8 of the chemical measures (which must be in error)
* **IMPUTE missing values** for variables (except `STARS`) using the median
* **Created missing `STARS` variable** for 26.25% of cases that have a missing `STARS` value
* **Drop variables** that have no correlation to `TARGET` even after above transformations
* **LOG transformation* for all of the numeric chemical variables that were not normally distributed




### Change Data Type of `STARS`

We set the `STARS` variable as a factor as it's a rating and not a continuous scale.


```{r}
raw$STARS <- as.factor(raw$STARS)
```

### Bucket `LabelAppeal` Ratings

Switch `LabelAppeal` to a factor after coding the ratings into buckets of Negative, Neutral, and Positive. This will be easier to interpret as well. The boxplots after this change show how increased `LabelAppeal` seems to be positively associated with the `TARGET`.


```{r}
raw$LabelAppeal_factor <- ifelse(raw$LabelAppeal < 0,1,ifelse(raw$LabelAppeal==0,2,3))
raw$LabelAppeal_factor <- as.factor(raw$LabelAppeal_factor)
levels(raw$LabelAppeal_factor) <- c('Negative','Neutral','Positive')

# boxplot of new 3-level factors against target
raw %>%
  ggplot(aes(LabelAppeal_factor,TARGET))+
  geom_boxplot()+
  theme_classic()
```


### Fix Negative Values

With 21,766 negative values across many of the chemical variables, for which it doesn't make sense to have a negative value, we convert these to positive values so we retain some value (as opposed to omitting these measures). Variables include: `FixedAcidity`, `VolatileAcidity`, `CitricAcid`, `ResidualSugar`, `Chlorides`, `FreeSulfurDioxide`, `TotalSulfurDioxide`, `Sulphates`, `Alcohol`.

```{r}
## If these variables are not correlated with the Target it might make sense to just not use them. In theory the acidity variables should make up the AcidIndex variable and therefore it could make sense to not include the individual acidity variables.

# take absolute value of chemical variables that had negative values
raw$FixedAcidity <- abs(raw$FixedAcidity)
raw$VolatileAcidity <- abs(raw$VolatileAcidity)
raw$CitricAcid <- abs(raw$CitricAcid)
raw$ResidualSugar <- abs(raw$ResidualSugar)
raw$Chlorides <- abs(raw$Chlorides)
raw$FreeSulfurDioxide <- abs(raw$FreeSulfurDioxide)
raw$TotalSulfurDioxide <- abs(raw$TotalSulfurDioxide)
raw$Sulphates <- abs(raw$Sulphates)
raw$Alcohol <- abs(raw$Alcohol)
```


### Impute Missing Values

For the 7 variables that had <10% missing values, we will impute the missing values with the median for that variable. Variables include: `ResidualSugar`, `Chlorides`, `FreeSulfurDioxide`, `TotalSulfurDioxide`,  `pH`, `Sulphates`, `Alcohol`.

```{r}
raw <- raw %>%
  mutate(
  ResidualSugar = ifelse(is.na(ResidualSugar), median(ResidualSugar, na.rm = T), ResidualSugar),
  Chlorides = ifelse(is.na(Chlorides), median(Chlorides, na.rm = T), Chlorides),
  FreeSulfurDioxide = ifelse(is.na(FreeSulfurDioxide), median(FreeSulfurDioxide, na.rm = T), FreeSulfurDioxide),
  TotalSulfurDioxide = ifelse(is.na(TotalSulfurDioxide), median(TotalSulfurDioxide, na.rm = T), TotalSulfurDioxide),
  pH = ifelse(is.na(pH), median(pH, na.rm = T), pH),
  Sulphates = ifelse(is.na(Sulphates), median(Sulphates, na.rm = T), Sulphates),
  Alcohol = ifelse(is.na(Alcohol), median(Alcohol, na.rm = T), Alcohol))
```


### Flag `STARS` Missing Values
With 26.25% of cases missing a `STARS` rating, we need to consider if we should drop the variable from modeling, impute the missing values, or flag these cases. There are some guidelines in data science that if there are beyond 30% missing values, the variable may be best dropped. Theoretically, the `STARS` variable should represent something close to our `TARGET` variable as we'd expect a relationship between high expert reviews and high case sales. Since this seems like too valuable of a variable to drop, and considering that depending on how the `STARS` data was obtained, a missing value could indicate a lesser quality or less popular wine (such that it hasn't been rated by experts), we choose to create a new variable where a '1' indicates a missing `STARS` value. We leave the NAs in the original `STARS` variable. We set the `STARS_new` variable as a factor.

```{r}
# create new variable to flag if it's a missing value
raw <- raw %>%
  mutate(
    STARS_new = ifelse(is.na(STARS),0,STARS)
  )

# set as factor
raw$STARS_new <- as.factor(raw$STARS_new)
levels(raw$STARS_new) <- c('no rating','one star','two star','three star','four star')

# check boxplots for new variable against target
raw %>%
  ggplot(aes(STARS_new,TARGET)) +
  geom_boxplot()+
  theme_classic()
```

### Drop Variables

The correlation plot below show that, even after some of the above transformations, the majority of the numeric variables are not correlated with the `TARGET`. We choose to drop `FixedAcidity`, `VolatileAcidity`, `CitricAcid` as theoritically they should be represented in the `AcidIndex`. 


```{r}
#remove factor variables so corrplot will run
raw_numeric <- dplyr::select(raw, -c('STARS_new', 'LabelAppeal_factor', 'STARS'))

# create corr values
correlation2 <- cor(raw_numeric, use = "complete.obs") 

# plot correlations
corrplot.mixed(correlation2, tl.col = 'black', tl.pos = 'lt', number.cex= 11/ncol(raw_numeric))

# drop 3 acid variables
raw <- dplyr::select(raw, -c('FixedAcidity', 'VolatileAcidity', 'CitricAcid'))

```



### Preform Log Transformations

The normality plots earlier identified the following variables could benefit from log transformations, which is even more true after having taken the absolute value of many of these variables (as a result of un-interpretable negative chemical values). We also add 0.00000001 to the new value in order to avoid any values of zero for which there is no defined log value while not affect the variables distribution greatly.

```{r}
raw$log_ResidualSugar <- log(raw$ResidualSugar + 0.00000001)
raw$log_Chlorides <- log(raw$Chlorides + 0.00000001)
raw$log_FreeSulfurDioxide <- log(raw$FreeSulfurDioxide + 0.00000001)
raw$TotalSulfurDioxide <-  log(raw$TotalSulfurDioxide + 0.00000001)
raw$log_Density <- log(raw$Density + 0.00000001)
raw$log_PH <- log(raw$pH + 0.00000001)
raw$log_Sulphates <- log(raw$Sulphates + 0.00000001)
raw$log_Alcohol <- log(raw$Alcohol + 0.00000001)
```



### Re-Run Normality, Outlier, & Correlation Plots

After all of the above changes, we check the dataset below. We've dropped the pre-logging variables for simplicity. 

```{r}
# drop original variables that are now logged
raw <- dplyr::select(raw, -c('ResidualSugar', 'Chlorides', 'FreeSulfurDioxide',
                                     'TotalSulfurDioxide', 'Density', 'pH', 'Sulphates', 'Alcohol'))

# summary plot
raw %>%
    diagnose_numeric()%>%
    dplyr::select(variables, min, mean, median, max, zero, minus)%>%
    flextable(theme_fun = theme_booktabs()) %>%
    colformat_double(digits = 2)
```

Next we check the outlier table to see if we still have great than 5% of data points identified as outliers in a large proportion of the chemical variables.  It appears we have normalized some of the outliers with the above transformations, but `log_Density`, `log_PH`, `log_Alcohol`, and `AcidIndex` still have more than 5% of values identified as outliers.

#### is there anything else we can/should do for these variables with high proportion of outliers? -rachel


```{r}
diagnose_outlier(raw) %>%
  arrange(desc(outliers_cnt)) %>% 
  mutate_if(is.numeric, round , digits=3) %>% 
  flextable() 
```


Looking at the normality plots for the 7 variables  for which we converted negative values into positive values and then performed a log transformation, they are more normal than they were previously, but some are still seriously skewed.

```{r}
raw %>%
  dplyr::select(log_ResidualSugar,log_Chlorides, log_FreeSulfurDioxide, 
         log_Density, log_PH, log_Sulphates, log_Alcohol) %>%
  plot_normality()

```


Creating a new correlation plot with our transformed numeric variables it appears there is very little correlation between our `TARGET` variable and any of these chemical measures with the exception of `AcidIndex`. At this point the `LabelAppeal`, `AcidIndex`, and earlier we saw thee `STARS` variable have the strongest correlations with the `TARGET`.


```{r}
#remove factor variables so corrplot will run
raw_numeric <- dplyr::select(raw, -c('STARS_new', 'LabelAppeal_factor', 'STARS'))

# create corr values
correlation2 <- cor(raw_numeric, use = "complete.obs") 

# plot correlations
corrplot.mixed(correlation2, tl.col = 'black', tl.pos = 'lt', number.cex= 11/ncol(raw_numeric))
```
## Build Models

Our prompt asks us to build at least two of three different models: *poisson*, *negative binomial*, and *multiple linear regression*. As there appears to be no correlation from the logged chemical variables, we won't include those in any of the following models.

### because we left the NAs in the STARS original variable it is dropping all of those cases when we include that variable, what's a good way to get around that?

### The Poissons

Poisson distributions are used for count data and relies on the distribution generally have a mean = variance. Poisson is particularly useful when the counts are small, if they are larger we may be able to get use out of a linear regression (which we'll try later).

#### is there a good way to check if variance = mean in our dataset? this appears to be the big-deal thing with poisson - rachel


using this link for interpretation and quasi-poisson method: https://www.dataquest.io/blog/tutorial-poisson-regression-in-r/

First we model based on the `AcidIndex`, `STAR_new`, and `LabelAppeal_factor` variables. It appears all have significant p-values. We see a residual deviance of 13,776 which is greater than the 12,787 degrees of freedom - this suggests over-dispersion exists. Our estimates are correct, but the standard errors are wrong and the model isn't accounting for them. Finally, comparing the null deviance (a model with only the intercept) we see that we have a greatly lower residual deviance for  the 8 degrees of freedom we lost in adding our predictive variables. This means the model we built fits better than the null.


```{r}
m_pois_1 <- glm(TARGET ~ AcidIndex + STARS_new-1 + LabelAppeal_factor, family = "poisson", raw)
summary(m_pois_1)
```

Because of the overdispersion discovered about, we try a quasipoisson model, which accounts for this. We will, however, lose having a AIC value.

```{r}

m_qpois_1 <- glm(TARGET ~ AcidIndex + STARS_new-1 + LabelAppeal_factor, family = quasipoisson(link = "log"), raw)
summary(m_qpois_1)

```

The model below has a higher AIC than m_pois_1 and is also over-dispersed, so we will not proceed with trying this as a quasipoisson.

```{r}
m_pois_2 <- glm(TARGET ~ STARS_new-1, family = "poisson", raw)
summary(m_pois_2)
```

The model below shows that `log_ResidualSugar`, `log_Density`, `log_PH`, and `log_Sulphates` do not have significant p-values. Despite low values in the correlation plots, it does appear that `log_Chlorides`, `log_FreeSulfurDioxide`, and `log_Alcohol` are significant at this stage. We again have a larger residual deviance than the residual degrees of freedom, so will explore a quasipoisson model after using backwards selection.

```{r}
## all variables, except original STARS (so it doesn't drop NAs)
m_pois_3 <- glm(TARGET ~ ., family = "poisson", raw[,c(1,3,5:13)])
summary(m_pois_3)
```

```{r}
# try backwards step from previous mdoel
m_pois_4 <- step(m_pois_3, selection='backwards')
summary(m_pois_4)
```

After seein the variables selected above, and the overdispersion, we build a last quasipoisson model that has the variarbles the backwards selection isolated.

```{r}
m_qpois_4 <- glm(TARGET ~ AcidIndex + LabelAppeal_factor + STARS_new + log_Chlorides +
                   log_FreeSulfurDioxide + log_Sulphates + log_Alcohol, 
                 family = quasipoisson(link = "log"), raw)
summary(m_qpois_4)
```




### The Negative Binomials

Using a negative binomial model for the same variables we selected in the first poisson model.

```{r}
m_binom_1 <- glm.nb(TARGET ~ AcidIndex + STARS_new + LabelAppeal_factor, raw)
summary(m_binom_1)
```


Running a negative binomial model on just the STARS_new variable as we did in the second poisson model.

```{r}
m_binom_2 <- glm.nb(TARGET ~ STARS_new, raw)
summary(m_binom_2)
```

Running a negative binomial model on all variarbles except the original `STARS` variable before it was recoded into `STARS_new`.

```{r}
m_binom_3 <- glm.nb(TARGET ~ ., raw[,c(1,3,5:13)])
summary(m_binom_3)
```

Performing backwards selectino on the above binomial model.

```{r}
m_binom_4 <- step(m_binom_3, selection='backwards')
summary(m_binom_4)
```


### The Multiple Linears  

Theoretically, a linear regression is not a good choice for count data and further not a good choice when we have had some troubles with this dataset with respect to normality.


```{r}
m_mlin_1 <- lm(TARGET ~ AcidIndex + STARS_new + LabelAppeal_factor, raw)
summary(m_mlin_1)
```



```{r}
m_mlin_2 <- lm(TARGET ~ ., raw[,c(1,3,5:13)])
summary(m_mlin_2)
```

```{r}
m_mlin_3 <- step(m_mlin_2, selection='backwards')
summary(m_mlin_3)
```



## Model Selection








